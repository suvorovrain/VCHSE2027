\section{Лекция 11 19.05.25}

Численные методы решения ОДУ

\subsection{Задача Коши}
Рассматривается задача Коши для обыкновенного дифференциального уравнения (ОДУ) первого порядка:
\[ y' = f(x, y), \quad x \in [a, b] \]
с начальным условием:
\[ y(a) = y_a \]
Ищется решение $y = y(x)$.

\textbf{Условие существования и единственности ($\exists!$) решения:}
Функция $f(x,y)$ непрерывна по $x$ и удовлетворяет условию Липшица по $y$:
\[ |f(x, y_1) - f(x, y_2)| \le L |y_1 - y_2|, \quad L = \text{const} > 0 \]

\subsection{Метод последовательных приближений (метод Пикара?)}
Решение задачи Коши может быть представлено в интегральной форме:
\[ y(x) = y_a + \int_a^x f(s, y(s)) ds\]
Строится последовательность функций (итераций):
\[ y_0(x) = y_a \]
\[ y_n(x) = y_a + \int_a^x f(s, y_{n-1}(s)) ds, \quad n=1, 2, \dots \]
Этот метод имеет теоретическую ценность (используется в теореме существования и единственности), но на практике используется редко из-за сложности вычисления интегралов.

\subsection{Методы на основе рядов Тейлора (Метод степенных рядов)}
Вводится равномерная сетка узлов $x_j$ на отрезке $[a,b]$ с шагом $h$: $x_j = a+jh$.
Пусть $y_j \approx y(x_j)$ --- численное решение в узле $x_j$.
Если $y_j$ известно, $y_{j+1}$ можно найти, используя разложение в ряд Тейлора:
\[ y(x_{j+1}) = y(x_j + h) = y(x_j) + h y'(x_j) + \frac{h^2}{2!} y''(x_j) + \dots + \frac{h^{k}}{k!} y^{(k)}(x_j) + \mathcal{O}(h^{k+1}) \]
Производные $y^{(k)}(x_j)$ находятся из исходного ОДУ $y' = f(x,y)$ путем последовательного дифференцирования:
\[ y'(x_j) = f(x_j, y(x_j)) \]
\[ y''(x_j) = \left( \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} f \right)(x_j, y(x_j)) \]
и т.д.

\subsection{Метод Эйлера}
Отбрасывая в разложении Тейлора члены порядка $h^2$ и выше, и заменяя $y(x_j)$ на $y_j$, $y'(x_j)$ на $f(x_j, y_j)$, получаем формулу метода Эйлера:
\[ y_{j+1} = y_j + h f(x_j, y_j) \]
Это явный одношаговый метод.

\subsubsection{Погрешность метода Эйлера}
Локальная погрешность метода Эйлера (погрешность на одном шаге, предполагая $y_j=y(x_j)$):
\[ \delta_j = y(x_{j+1}) - (y_j + h f(x_j, y_j)) = \left( y(x_j) + h y'(x_j) + \frac{h^2}{2} y''(\xi_j) \right) - (y(x_j) + h y'(x_j)) = \frac{h^2}{2} y''(\xi_j) \]
Таким образом, локальная погрешность метода Эйлера есть $\mathcal{O}(h^2)$.

\textbf{Глобальная погрешность:}
Пусть $e_j = y(x_j) - y_j$ --- глобальная погрешность в узле $x_j$.
\[ e_{j+1} = y(x_{j+1}) - y_{j+1} = \left( y(x_j) + h f(x_j, y(x_j)) + \frac{h^2}{2} y''(\xi_j) \right) - (y_j + h f(x_j, y_j)) \]
\[ e_{j+1} = (y(x_j) - y_j) + h (f(x_j, y(x_j)) - f(x_j, y_j)) + \frac{h^2}{2} y''(\xi_j) \]
Используя теорему о среднем для разности $f$: $f(x_j, y(x_j)) - f(x_j, y_j) = \frac{\partial f}{\partial y}(x_j, \bar{y}_j) (y(x_j) - y_j) = \frac{\partial f}{\partial y}(x_j, \bar{y}_j) e_j$.
Пусть $\left|\frac{\partial f}{\partial y}\right| \le L$ (условие Липшица) и $|y''(x)| \le M_2$.
\[ |e_{j+1}| \le |e_j| + h L |e_j| + \frac{M_2}{2} h^2 = |e_j|(1+hL) + \frac{M_2}{2} h^2 \]

\textbf{Теорема (о глобальной погрешности метода Эйлера):}
При некоторых ограничениях на функцию $f$ и ее производные, глобальная погрешность метода Эйлера есть $\mathcal{O}(h)$. Более точно:
\[ |e_N| \le \frac{M_2 h}{2L} (e^{L(b-a)} - 1) \]

\begin{proof} (набросок)
Используем рекуррентное соотношение $|e_{j+1}| \le (1+hL)|e_j| + C h^2$, где $C = M_2/2$. Начальная погрешность $e_0 = y(a) - y_a = 0$.
По индукции можно показать, что:
\[ |e_j| \le \frac{C h}{L} (e^{Ljh} - 1) \]
Действительно, для $j=0$, $|e_0|=0$.
Предположим, что это верно для $j=m$: $|e_m| \le \frac{C h}{L} (e^{Lmh} - 1)$.
Тогда для $m+1$:
\begin{align*} |e_{m+1}| &\le (1+hL)|e_m| + C h^2 \\ &\le (1+hL) \frac{C h}{L} (e^{Lmh} - 1) + C h^2 \\ &= \frac{C h}{L} (1+hL)e^{Lmh} - \frac{C h}{L}(1+hL) + C h^2 \\ &= \frac{C h}{L} (1+hL)e^{Lmh} - \frac{C h}{L} - C h^2 + C h^2 \\ &= \frac{C h}{L} ((1+hL)e^{Lmh} - 1) \end{align*}
Так как $1+hL \le e^{hL}$:
\[ |e_{m+1}| \le \frac{C h}{L} (e^{hL}e^{Lmh} - 1) = \frac{C h}{L} (e^{L(m+1)h} - 1) \]
Утверждение доказано по индукции.
Для $j=N$, $Nh = b-a$.
\[ |e_N| \le \frac{C h}{L} (e^{L(b-a)} - 1) = \frac{M_2 h}{2L} (e^{L(b-a)} - 1) = \mathcal{O}(h) \]
Следовательно, метод Эйлера имеет первый порядок точности.
\end{proof}

\subsection{Модифицированный метод Эйлера (метод Хойна / улучшенный метод Эйлера)}
Для повышения точности можно использовать информацию о производной в нескольких точках.
\begin{enumerate}
    \item \textbf{Прогноз:} вычисляется предварительное значение $y_{j+1}^*$ с помощью метода Эйлера:
    \[ y_{j+1}^* = y_j + h f(x_j, y_j) \]
    \item \textbf{Коррекция:} используется среднее арифметическое наклонов в точке $(x_j,y_j)$ и $(x_{j+1}, y_{j+1}^*)$:
    \[ y_{j+1} = y_j + \frac{h}{2} [f(x_j, y_j) + f(x_{j+1}, y_{j+1}^*)] \]
\end{enumerate}
При некоторых ограничениях на $f$ и ее производные, глобальная погрешность этого метода составляет $\mathcal{O}(h^2)$.

\subsection{Методы Рунге-Кутты}
Это семейство одношаговых многостадийных методов.
\subsubsection{Двухстадийные методы Рунге-Кутты}
Общий вид:
\begin{align*} y_{j+1} &= y_j + A_1 k_1 + A_2 k_2 \\ k_1 &= h f(x_j, y_j) \\ k_2 &= h f(x_j + B_1 h, y_j + B_2 k_1) \end{align*}
Коэффициенты $A_1, A_2, B_1, B_2$ подбираются так, чтобы разложение $y_{j+1}$ по степеням $h$ совпадало с разложением Тейлора для точного решения $y(x_{j+1})$ до членов с $h^2$ включительно.
Точное решение: $y(x_j+h) = y(x_j) + h f(x_j,y_j) + \frac{h^2}{2} \left( \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} f \right)_{ (x_j,y_j) } + \mathcal{O}(h^3)$.
Разлагая $k_2$ в ряд Тейлора по $h$:
$k_2 = h \left[ f(x_j,y_j) + B_1 h \left(\frac{\partial f}{\partial x}\right)_j + B_2 k_1 \left(\frac{\partial f}{\partial y}\right)_j + \mathcal{O}(h^2) \right]$

Сравнивая коэффициенты при $h f_j$, $h^2 (f_x)_j$, $h^2 f_j (f_y)_j$, получаем систему уравнений:
\[ \begin{cases} A_1 + A_2 = 1 \\ A_2 B_1 = 1/2 \\ A_2 B_2 = 1/2 \end{cases} \]

\textbf{Пример: Метод Хойна (улучшенный метод Эйлера):}
Выбираем $A_1 = 1/2, A_2 = 1/2$. Тогда $B_1 = 1, B_2 = 1$.
\begin{align*} k_1 &= h f(x_j, y_j) \\ k_2 &= h f(x_j + h, y_j + k_1) \\ y_{j+1} &= y_j + \frac{1}{2} (k_1 + k_2) \end{align*}. Имеет ($\mathcal{O}(h^2)$).

\subsubsection{Метод Рунге-Кутты (трехстадийный)}
Один из распространенных трехстадийных методов 3-го порядка (метод Кутты):
\begin{align*} y_{j+1} &= y_j + \frac{1}{6} (k_1 + 4k_2 + k_3) \\ k_1 &= h f(x_j, y_j) \\ k_2 &= h f(x_j + h/2, y_j + k_1/2) \\ k_3 &= h f(x_j + h, y_j - k_1 + 2k_2) \end{align*}
Этот метод имеет третий порядок точности ($\mathcal{O}(h^3)$).

\subsubsection{Классический метод Рунге-Кутты (четырехстадийный)}
На практике часто используется классический метод Рунге-Кутты 4-го порядка:
\begin{align*} y_{j+1} &= y_j + \frac{1}{6} (k_1 + 2k_2 + 2k_3 + k_4) \\ k_1 &= h f(x_j, y_j) \\ k_2 &= h f(x_j + h/2, y_j + k_1/2) \\ k_3 &= h f(x_j + h/2, y_j + k_2/2) \\ k_4 &= h f(x_j + h, y_j + k_3) \end{align*}
Этот метод имеет четвертый порядок точности ($\mathcal{O}(h^4)$).


\subsection{Многошаговые методы Адамса}
Используется информация из нескольких предыдущих точек для аппроксимации интеграла:
\[ y(x_{j+1}) = y(x_j) + \int_{x_j}^{x_{j+1}} f(s, y(s)) ds \]
Функция $f(s, y(s))$ заменяется интерполяционным многочленом $L(s)$.

\subsubsection{Неявные методы Адамса}
Интерполяционный многочлен $L(s)$ строится по узлам $x_{j+1}, x_j, \dots, x_{j-M+1}$. Это приводит к нелинейному уравнению относительно $y_{j+1}$.
\begin{itemize}
    \item $M=1$ (2 точки, $f_{j+1}, f_j$):
    $L_1(s)$ --- линейный многочлен. $\int_{x_j}^{x_{j+1}} L_1(s) ds = \frac{h}{2}(f_{j+1} + f_j)$.
    \[ y_{j+1} = y_j + \frac{h}{2} (f(x_{j+1}, y_{j+1}) + f_j) \quad \text{(Метод трапеций, 2-й порядок)} \]
    \item $M=2$ (3 точки, $f_{j+1}, f_j, f_{j-1}$):
    \[ y_{j+1} = y_j + \frac{h}{12} (5f(x_{j+1}, y_{j+1}) + 8f_j - f_{j-1}) \quad \text{(3-й порядок)} \]
    \item $M=3$ (4 точки, $f_{j+1}, f_j, f_{j-1}, f_{j-2}$):
    \[ y_{j+1} = y_j + \frac{h}{24} (9f(x_{j+1}, y_{j+1}) + 19f_j - 5f_{j-1} + f_{j-2}) \quad \text{(4-й порядок)} \]
\end{itemize}
Неявные методы обычно более точны и устойчивы, чем явные того же числа шагов.

\subsubsection{Методы прогноза-коррекции (Предиктор-Корректор)}
Комбинируют явный и неявный методы.
\begin{enumerate}
    \item \textbf{Шаг прогноза (Предиктор):} Вычисляется начальное приближение $y_{j+1}^{(0)}$ с помощью явного метода.
    \[ y_{j+1}^{(0)} = y_j + \frac{h}{2} (3f_j - f_{j-1}) \]
    \item \textbf{Шаг коррекции (Корректор):} Уточняется значение $y_{j+1}$ с помощью неявного метода, используя $y_{j+1}^{(k)}$ в правой части. Уравнение решается итерационно.
    \[ y_{j+1}^{(k+1)} = y_j + \frac{h}{2} (f(x_{j+1}, y_{j+1}^{(k)}) + f_j) \]
    Итерации продолжаются до тех пор, пока $|y_{j+1}^{(k+1)} - y_{j+1}^{(k)}| < \varepsilon$.
\end{enumerate}
\textbf{Алгоритм прогноза-коррекции:}
\begin{itemize}
    \item Вычислить $y_{j+1}^{(0)}$ по явной формуле (прогноз).
    \item Вычислить $f(x_{j+1}, y_{j+1}^{(0)})$.
    \item Вычислить $y_{j+1}^{(1)}$ по неявной формуле, используя $f(x_{j+1}, y_{j+1}^{(0)})$ (одна итерация корректора).
\end{itemize}


\subsection{Конечно-разностные методы для краевых задач}
Рассмотрим краевую задачу для ОДУ второго порядка:
\[ y'' = F(x, y, y'), \quad x \in [a,b] \]
с граничными условиями:
\[ y(a) = y_A, \quad y(b) = y_B \]
Для простоты рассмотрим линейную задачу $y'' = F(x)$.
Введем равномерную сетку $x_j = a + jh$, $j=0, 1, \dots, N$, где $x_0=a, x_N=b, h=(b-a)/N$.
Заменим производную $y''(x_j)$ конечно-разностной аппроксимацией:
\[ y''(x_j) \approx \frac{y(x_{j+1}) - 2y(x_j) + y(x_{j-1})}{h^2} \]
Заменяя точные значения $y(x_j)$ на приближенные $y_j$, получаем систему уравнений:
\[ \frac{y_{j+1} - 2y_j + y_{j-1}}{h^2} = F(x_j), \quad j = 1, 2, \dots, N-1 \]
Граничные условия дают:
\[ y_0 = y_A \]
\[ y_N = y_B \]
Перепишем систему:
\[ y_{j-1} - 2y_j + y_{j+1} = h^2 F_j, \quad \text{где } F_j = F(x_j) \]
Для $j=1$: $y_0 - 2y_1 + y_2 = h^2 F_1 \Rightarrow -2y_1 + y_2 = h^2 F_1 - y_A$.
Для $j=N-1$: $y_{N-2} - 2y_{N-1} + y_N = h^2 F_{N-1} \Rightarrow y_{N-2} - 2y_{N-1} = h^2 F_{N-1} - y_B$.

Это система линейных алгебраических уравнений (СЛАУ) относительно неизвестных $y_1, y_2, \dots, y_{N-1}$.  
